"""
Video Cinematic Processor - GPU BF16 å¤šçº¿ç¨‹è§†é¢‘å¤„ç†
è§†é¢‘æ‹†å¸§ â†’ ç”»è´¨å¢å¼º â†’ è¡¥å¸§ â†’ åˆæˆè§†é¢‘

åŠŸèƒ½ï¼š
1. è§†é¢‘æ‹†å¸§ï¼šæ”¯æŒä»»æ„è§†é¢‘æ ¼å¼
2. GPU BF16å¤„ç†ï¼šè¾¹ç¼˜é”åŒ– + å…‰æ„Ÿå¢å¼º
3. è¡¥å¸§ï¼šRIFEå…‰æµè¡¥å¸§ 2x/4x
4. è§†é¢‘åˆæˆï¼šæ”¯æŒå¤šç§ç¼–ç æ ¼å¼
"""

import torch
import torch.nn.functional as F
import numpy as np
import os
import tempfile
import subprocess
from concurrent.futures import ThreadPoolExecutor
from typing import Tuple, List, Optional
import folder_paths


class VideoCinematicProcessor:
    """è§†é¢‘ç”»è´¨å¢å¼ºå¤„ç†å™¨ - GPU BF16å¤šçº¿ç¨‹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video": ("VIDEO",),
                "sharpness": ("FLOAT", {
                    "default": 0.5,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "è¾¹ç¼˜é”åŒ–å¼ºåº¦"
                }),
                "luminosity": ("FLOAT", {
                    "default": 0.3,
                    "min": 0.0,
                    "max": 1.0,
                    "step": 0.1,
                    "display": "slider",
                    "tooltip": "å…‰æ„Ÿå±‚æ¬¡å¼ºåº¦"
                }),
                "frame_interpolation": (["none", "2x", "4x"], {
                    "default": "none",
                    "tooltip": "è¡¥å¸§å€æ•° (RIFEå…‰æµ)"
                }),
            },
            "optional": {
                "shadow_lift": ("FLOAT", {
                    "default": 0.15,
                    "min": 0.0,
                    "max": 0.5,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "é˜´å½±æäº®"
                }),
                "highlight_roll": ("FLOAT", {
                    "default": 0.1,
                    "min": 0.0,
                    "max": 0.5,
                    "step": 0.05,
                    "display": "slider",
                    "tooltip": "é«˜å…‰å›é€€"
                }),
                "batch_size": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 32,
                    "step": 1,
                    "tooltip": "GPUæ‰¹å¤„ç†å¤§å°"
                }),
                "num_workers": ("INT", {
                    "default": 4,
                    "min": 1,
                    "max": 16,
                    "step": 1,
                    "tooltip": "å¤šçº¿ç¨‹å·¥ä½œæ•°"
                }),
            },
        }

    RETURN_TYPES = ("VIDEO",)
    RETURN_NAMES = ("video",)
    FUNCTION = "process_video"
    CATEGORY = "None-upup"

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        self.rife_model = None

    def process_video(self, video, sharpness, luminosity, frame_interpolation="none",
                      shadow_lift=0.15, highlight_roll=0.1, batch_size=4, num_workers=4):
        """
        ä¸»å¤„ç†æµç¨‹ï¼šæ‹†å¸§ â†’ å¢å¼º â†’ è¡¥å¸§ â†’ åˆæˆ
        """
        # Step 1: æ‹†å¸§
        frames, fps, audio_path = self._extract_frames(video)
        original_fps = fps
        
        # Step 2: GPU BF16æ‰¹é‡å¤„ç†
        enhanced_frames = self._batch_enhance_gpu(
            frames, sharpness, luminosity, shadow_lift, highlight_roll, batch_size
        )
        
        # Step 3: è¡¥å¸§
        if frame_interpolation != "none":
            multiplier = 2 if frame_interpolation == "2x" else 4
            enhanced_frames = self._interpolate_frames(enhanced_frames, multiplier, batch_size)
            fps = original_fps * multiplier
        
        # Step 4: åˆæˆè§†é¢‘
        output_video = self._compose_video(enhanced_frames, fps, audio_path)
        
        return (output_video,)

    def _extract_frames(self, video) -> Tuple[torch.Tensor, float, Optional[str]]:
        """
        ä»è§†é¢‘ä¸­æå–å¸§
        è¿”å›: (frames_tensor [N,H,W,C], fps, audio_path)
        """
        import cv2
        
        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„
        if isinstance(video, str):
            video_path = video
        elif hasattr(video, 'path'):
            video_path = video.path
        else:
            # å‡è®¾æ˜¯tensoræ ¼å¼ [N,H,W,C]
            return video, 30.0, None
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        frames = []
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            # BGR -> RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        cap.release()
        
        # æå–éŸ³é¢‘
        audio_path = self._extract_audio(video_path)
        
        # è½¬æ¢ä¸ºtensor [N,H,W,C] float32 0-1
        frames_np = np.stack(frames, axis=0).astype(np.float32) / 255.0
        frames_tensor = torch.from_numpy(frames_np)
        
        return frames_tensor, fps, audio_path

    def _extract_audio(self, video_path: str) -> Optional[str]:
        """æå–éŸ³é¢‘è½¨é“"""
        try:
            audio_path = os.path.join(tempfile.gettempdir(), "temp_audio.aac")
            cmd = [
                "ffmpeg", "-y", "-i", video_path,
                "-vn", "-acodec", "copy", audio_path
            ]
            subprocess.run(cmd, capture_output=True, check=True)
            return audio_path if os.path.exists(audio_path) else None
        except:
            return None

    def _batch_enhance_gpu(self, frames: torch.Tensor, sharpness: float, 
                           luminosity: float, shadow_lift: float, 
                           highlight_roll: float, batch_size: int) -> torch.Tensor:
        """
        GPU BF16æ‰¹é‡å¢å¼ºå¤„ç†
        frames: [N, H, W, C]
        """
        N, H, W, C = frames.shape
        results = []
        
        # è½¬æ¢ä¸º [N, C, H, W] æ ¼å¼ç”¨äºGPUå¤„ç†
        frames_gpu = frames.permute(0, 3, 1, 2).to(self.device, dtype=self.dtype)
        
        for i in range(0, N, batch_size):
            batch = frames_gpu[i:i+batch_size]
            
            # è¾¹ç¼˜æ„ŸçŸ¥é”åŒ– (GPU)
            if sharpness > 0:
                batch = self._edge_sharpen_gpu(batch, sharpness)
            
            # å…‰æ„Ÿå¢å¼º (GPU)
            if luminosity > 0:
                batch = self._luminosity_enhance_gpu(batch, luminosity, shadow_lift, highlight_roll)
            
            results.append(batch.cpu())
            
            # æ¸…ç†GPUå†…å­˜
            if i % (batch_size * 4) == 0:
                torch.cuda.empty_cache()
        
        # åˆå¹¶å¹¶è½¬å› [N, H, W, C]
        enhanced = torch.cat(results, dim=0)
        enhanced = enhanced.permute(0, 2, 3, 1).to(torch.float32)
        enhanced = torch.clamp(enhanced, 0, 1)
        
        return enhanced

    def _edge_sharpen_gpu(self, batch: torch.Tensor, sharpness: float) -> torch.Tensor:
        """
        GPUè¾¹ç¼˜æ„ŸçŸ¥é”åŒ–
        batch: [B, C, H, W] BF16
        """
        # Sobelè¾¹ç¼˜æ£€æµ‹æ ¸
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], 
                               dtype=self.dtype, device=self.device).view(1, 1, 3, 3)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], 
                               dtype=self.dtype, device=self.device).view(1, 1, 3, 3)
        
        # è½¬ç°åº¦è®¡ç®—è¾¹ç¼˜
        gray = batch.mean(dim=1, keepdim=True)
        
        # è¾¹ç¼˜æ£€æµ‹
        grad_x = F.conv2d(gray, sobel_x, padding=1)
        grad_y = F.conv2d(gray, sobel_y, padding=1)
        gradient = torch.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–è¾¹ç¼˜æ©ç 
        grad_max = gradient.amax(dim=(2, 3), keepdim=True) + 1e-6
        edge_mask = gradient / grad_max
        edge_mask = torch.clamp((edge_mask - 0.1) / 0.9, 0, 1)
        
        # é«˜æ–¯æ¨¡ç³Š
        gaussian_kernel = self._get_gaussian_kernel(5, 0.8).to(self.device, dtype=self.dtype)
        edge_mask = F.conv2d(edge_mask, gaussian_kernel, padding=2)
        
        # USMé”åŒ–
        blur = F.conv2d(batch, gaussian_kernel.repeat(3, 1, 1, 1), padding=2, groups=3)
        detail = batch - blur
        detail = torch.clamp(detail, -0.2, 0.2)  # é™å¹…
        
        amount = 0.5 + sharpness * 1.5
        sharpened = batch + detail * amount * edge_mask
        
        return sharpened

    def _luminosity_enhance_gpu(self, batch: torch.Tensor, intensity: float,
                                shadow_lift: float, highlight_roll: float) -> torch.Tensor:
        """
        GPUäº®åº¦åŒºåŸŸå…‰æ„Ÿå¢å¼º
        batch: [B, C, H, W] BF16
        """
        # è®¡ç®—äº®åº¦é€šé“
        luminance = 0.299 * batch[:, 0:1] + 0.587 * batch[:, 1:2] + 0.114 * batch[:, 2:3]
        
        # ç”Ÿæˆäº®åº¦åŒºåŸŸæ©ç 
        shadows_mask = self._smoothstep_gpu(0.35, 0.20, luminance)
        highlights_mask = self._smoothstep_gpu(0.65, 0.80, luminance)
        midtones_mask = torch.clamp(1.0 - shadows_mask - highlights_mask, 0, 1)
        
        # é«˜æ–¯æ¨¡ç³Šæ©ç 
        blur_kernel = self._get_gaussian_kernel(7, 3.0).to(self.device, dtype=self.dtype)
        shadows_mask = F.conv2d(shadows_mask, blur_kernel, padding=3)
        highlights_mask = F.conv2d(highlights_mask, blur_kernel, padding=3)
        midtones_mask = F.conv2d(midtones_mask, blur_kernel, padding=3)
        
        # å¤§åŠå¾„æ¨¡ç³Šæå–ä½é¢‘
        large_blur_kernel = self._get_gaussian_kernel(25, 8.0).to(self.device, dtype=self.dtype)
        base = F.conv2d(batch, large_blur_kernel.repeat(3, 1, 1, 1), padding=12, groups=3)
        detail = batch - base
        
        # å„åŒºåŸŸç‹¬ç«‹å¢å¼º
        shadow_factor = 1.0 + shadow_lift * intensity * 2.0
        mid_factor = 1.0 + intensity * 0.8
        high_factor = 1.0 + highlight_roll * intensity * 0.5
        
        enhanced = (
            shadows_mask * (base + detail * shadow_factor + shadow_lift * intensity * 0.06) +
            midtones_mask * (base + detail * mid_factor) +
            highlights_mask * (base + detail * high_factor - highlight_roll * intensity * 0.04)
        )
        
        # ä¿æŒåŸå§‹äº®åº¦ä¸­å¿ƒ
        enhanced = enhanced - enhanced.mean(dim=(2, 3), keepdim=True) + batch.mean(dim=(2, 3), keepdim=True)
        
        return enhanced

    def _smoothstep_gpu(self, edge0: float, edge1: float, x: torch.Tensor) -> torch.Tensor:
        """GPUå¹³æ»‘è¿‡æ¸¡å‡½æ•°"""
        t = torch.clamp((x - edge0) / (edge1 - edge0 + 1e-6), 0, 1)
        return t * t * (3 - 2 * t)

    def _get_gaussian_kernel(self, size: int, sigma: float) -> torch.Tensor:
        """ç”Ÿæˆé«˜æ–¯å·ç§¯æ ¸"""
        coords = torch.arange(size, dtype=torch.float32) - size // 2
        g = torch.exp(-(coords**2) / (2 * sigma**2))
        g = g / g.sum()
        kernel = g.outer(g)
        return kernel.view(1, 1, size, size)

    def _interpolate_frames(self, frames: torch.Tensor, multiplier: int, 
                            batch_size: int) -> torch.Tensor:
        """
        RIFEå…‰æµè¡¥å¸§
        frames: [N, H, W, C]
        """
        if multiplier == 1:
            return frames
        
        # å°è¯•åŠ è½½RIFEæ¨¡å‹
        rife_model = self._load_rife_model()
        
        if rife_model is None:
            # å¦‚æœæ²¡æœ‰RIFEï¼Œä½¿ç”¨ç®€å•åŒçº¿æ€§æ’å€¼
            return self._simple_interpolate(frames, multiplier)
        
        N = frames.shape[0]
        interpolated = []
        
        # è½¬æ¢æ ¼å¼ [N, C, H, W]
        frames_gpu = frames.permute(0, 3, 1, 2).to(self.device, dtype=self.dtype)
        
        for i in range(N - 1):
            frame0 = frames_gpu[i:i+1]
            frame1 = frames_gpu[i+1:i+2]
            
            interpolated.append(frame0.cpu())
            
            # ç”Ÿæˆä¸­é—´å¸§
            for t in range(1, multiplier):
                timestep = t / multiplier
                with torch.no_grad():
                    mid_frame = rife_model(frame0, frame1, timestep)
                interpolated.append(mid_frame.cpu())
            
            if i % 10 == 0:
                torch.cuda.empty_cache()
        
        # æ·»åŠ æœ€åä¸€å¸§
        interpolated.append(frames_gpu[-1:].cpu())
        
        # åˆå¹¶å¹¶è½¬å› [N, H, W, C]
        result = torch.cat(interpolated, dim=0)
        result = result.permute(0, 2, 3, 1).to(torch.float32)
        
        return result

    def _simple_interpolate(self, frames: torch.Tensor, multiplier: int) -> torch.Tensor:
        """ç®€å•åŒçº¿æ€§è¡¥å¸§ (å¤‡ç”¨æ–¹æ¡ˆ)"""
        N, H, W, C = frames.shape
        interpolated = []
        
        for i in range(N - 1):
            frame0 = frames[i]
            frame1 = frames[i + 1]
            
            interpolated.append(frame0)
            
            for t in range(1, multiplier):
                alpha = t / multiplier
                mid_frame = (1 - alpha) * frame0 + alpha * frame1
                interpolated.append(mid_frame)
        
        interpolated.append(frames[-1])
        
        return torch.stack(interpolated, dim=0)

    def _load_rife_model(self):
        """åŠ è½½RIFEè¡¥å¸§æ¨¡å‹"""
        if self.rife_model is not None:
            return self.rife_model
        
        try:
            # å°è¯•ä»ComfyUI-Frame-InterpolationåŠ è½½
            from custom_nodes.ComfyUI_Frame_Interpolation.rife_model import RIFE
            self.rife_model = RIFE().to(self.device)
            self.rife_model.eval()
            return self.rife_model
        except ImportError:
            pass
        
        try:
            # å°è¯•ä»VFIåŠ è½½
            from custom_nodes.ComfyUI_VFI.rife import load_rife
            self.rife_model = load_rife().to(self.device)
            return self.rife_model
        except ImportError:
            pass
        
        print("[VideoCinematicProcessor] RIFEæ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨åŒçº¿æ€§æ’å€¼")
        return None

    def _compose_video(self, frames: torch.Tensor, fps: float, 
                       audio_path: Optional[str]) -> str:
        """
        åˆæˆè¾“å‡ºè§†é¢‘
        frames: [N, H, W, C] float32 0-1
        """
        import cv2
        
        output_dir = folder_paths.get_output_directory()
        output_path = os.path.join(output_dir, f"cinematic_video_{os.getpid()}.mp4")
        temp_video = os.path.join(tempfile.gettempdir(), "temp_video.mp4")
        
        N, H, W, C = frames.shape
        
        # OpenCVå†™å…¥è§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(temp_video, fourcc, fps, (W, H))
        
        for i in range(N):
            frame = (frames[i].numpy() * 255).astype(np.uint8)
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            writer.write(frame_bgr)
        
        writer.release()
        
        # ä½¿ç”¨FFmpegåˆå¹¶éŸ³é¢‘å¹¶ç¼–ç 
        if audio_path and os.path.exists(audio_path):
            cmd = [
                "ffmpeg", "-y",
                "-i", temp_video,
                "-i", audio_path,
                "-c:v", "libx264", "-preset", "medium", "-crf", "18",
                "-c:a", "aac", "-b:a", "192k",
                "-shortest",
                output_path
            ]
        else:
            cmd = [
                "ffmpeg", "-y",
                "-i", temp_video,
                "-c:v", "libx264", "-preset", "medium", "-crf", "18",
                output_path
            ]
        
        try:
            subprocess.run(cmd, capture_output=True, check=True)
        except subprocess.CalledProcessError:
            # å¦‚æœFFmpegå¤±è´¥ï¼Œç›´æ¥å¤åˆ¶ä¸´æ—¶æ–‡ä»¶
            import shutil
            shutil.copy(temp_video, output_path)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_video):
            os.remove(temp_video)
        if audio_path and os.path.exists(audio_path):
            os.remove(audio_path)
        
        return output_path


class VideoFrameExtractor:
    """è§†é¢‘æ‹†å¸§èŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "video": ("VIDEO",),
            },
            "optional": {
                "frame_skip": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 10,
                    "step": 1,
                    "tooltip": "è·³å¸§æ•° (0=å…¨éƒ¨å¸§)"
                }),
                "max_frames": ("INT", {
                    "default": 0,
                    "min": 0,
                    "max": 10000,
                    "step": 1,
                    "tooltip": "æœ€å¤§å¸§æ•° (0=æ— é™åˆ¶)"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE", "FLOAT", "INT")
    RETURN_NAMES = ("frames", "fps", "frame_count")
    FUNCTION = "extract"
    CATEGORY = "None-upup"

    def extract(self, video, frame_skip=0, max_frames=0):
        import cv2
        
        if isinstance(video, str):
            video_path = video
        elif hasattr(video, 'path'):
            video_path = video.path
        else:
            # å·²ç»æ˜¯tensor
            return (video, 30.0, video.shape[0])
        
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        frames = []
        frame_idx = 0
        skip_counter = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if skip_counter == 0:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
                
                if max_frames > 0 and len(frames) >= max_frames:
                    break
            
            skip_counter = (skip_counter + 1) % (frame_skip + 1)
            frame_idx += 1
        
        cap.release()
        
        frames_np = np.stack(frames, axis=0).astype(np.float32) / 255.0
        frames_tensor = torch.from_numpy(frames_np)
        
        return (frames_tensor, fps, len(frames))


class VideoFrameComposer:
    """è§†é¢‘åˆæˆèŠ‚ç‚¹"""
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "frames": ("IMAGE",),
                "fps": ("FLOAT", {
                    "default": 30.0,
                    "min": 1.0,
                    "max": 120.0,
                    "step": 0.1,
                }),
            },
            "optional": {
                "audio": ("AUDIO",),
                "codec": (["h264", "h265", "vp9"], {"default": "h264"}),
                "quality": ("INT", {
                    "default": 18,
                    "min": 0,
                    "max": 51,
                    "step": 1,
                    "tooltip": "CRFå€¼ (è¶Šä½è´¨é‡è¶Šé«˜)"
                }),
            },
        }

    RETURN_TYPES = ("VIDEO",)
    RETURN_NAMES = ("video",)
    FUNCTION = "compose"
    CATEGORY = "None-upup"

    def compose(self, frames, fps, audio=None, codec="h264", quality=18):
        import cv2
        
        output_dir = folder_paths.get_output_directory()
        output_path = os.path.join(output_dir, f"composed_video_{os.getpid()}.mp4")
        temp_video = os.path.join(tempfile.gettempdir(), "temp_compose.mp4")
        
        N, H, W, C = frames.shape
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        writer = cv2.VideoWriter(temp_video, fourcc, fps, (W, H))
        
        for i in range(N):
            frame = (frames[i].cpu().numpy() * 255).astype(np.uint8)
            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
            writer.write(frame_bgr)
        
        writer.release()
        
        # ç¼–ç å™¨æ˜ å°„
        codec_map = {
            "h264": "libx264",
            "h265": "libx265",
            "vp9": "libvpx-vp9"
        }
        
        cmd = [
            "ffmpeg", "-y",
            "-i", temp_video,
            "-c:v", codec_map[codec],
            "-crf", str(quality),
            "-preset", "medium",
        ]
        
        if audio is not None:
            audio_path = self._save_audio(audio)
            if audio_path:
                cmd.extend(["-i", audio_path, "-c:a", "aac", "-b:a", "192k", "-shortest"])
        
        cmd.append(output_path)
        
        try:
            subprocess.run(cmd, capture_output=True, check=True)
        except:
            import shutil
            shutil.copy(temp_video, output_path)
        
        if os.path.exists(temp_video):
            os.remove(temp_video)
        
        return (output_path,)

    def _save_audio(self, audio) -> Optional[str]:
        """ä¿å­˜éŸ³é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶"""
        try:
            import torchaudio
            audio_path = os.path.join(tempfile.gettempdir(), "temp_audio_compose.wav")
            torchaudio.save(audio_path, audio["waveform"], audio["sample_rate"])
            return audio_path
        except:
            return None


class GMFSSModelLoader:
    """
    GMFSS æ¨¡å‹åŠ è½½å™¨
    æ”¯æŒ GMFSS_Fortuna ç³»åˆ—æ¨¡å‹ (gmfss / union)
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "model_path": ("STRING", {
                    "default": "",
                    "tooltip": "GMFSSæ¨¡å‹è·¯å¾„ (.pklæ–‡ä»¶)"
                }),
                "model_type": (["gmfss", "union"], {
                    "default": "union",
                    "tooltip": "æ¨¡å‹ç±»å‹ï¼šgmfssåŸºç¡€ç‰ˆ / unionå¢å¼ºç‰ˆ"
                }),
            },
            "optional": {
                "scale": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.25,
                    "max": 2.0,
                    "step": 0.25,
                    "tooltip": "å…‰æµè®¡ç®—åˆ†è¾¨ç‡ç¼©æ”¾ (è¶Šå°è¶Šå¿«ï¼Œè´¨é‡é™ä½)"
                }),
            },
        }

    RETURN_TYPES = ("VFI_MODEL",)
    RETURN_NAMES = ("model",)
    FUNCTION = "load_model"
    CATEGORY = "None-upup"

    def load_model(self, model_path: str, model_type: str = "union", scale: float = 1.0):
        """åŠ è½½GMFSSæ¨¡å‹"""
        import importlib.util
        
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        
        # å°è¯•ä»modelsç›®å½•åŠ è½½
        if not model_path:
            models_dir = os.path.join(folder_paths.models_dir, "vfi")
            if model_type == "union":
                model_path = os.path.join(models_dir, "GMFSS_union.pkl")
            else:
                model_path = os.path.join(models_dir, "GMFSS.pkl")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"GMFSSæ¨¡å‹æœªæ‰¾åˆ°: {model_path}\nè¯·ä¸‹è½½æ¨¡å‹å¹¶æ”¾ç½®åˆ° models/vfi/ ç›®å½•")
        
        # åŠ è½½æ¨¡å‹
        model_data = {
            "type": "gmfss",
            "model_type": model_type,
            "model_path": model_path,
            "scale": scale,
            "device": device,
            "dtype": dtype,
            "model": None,  # å»¶è¿ŸåŠ è½½
        }
        
        print(f"[GMFSSModelLoader] æ¨¡å‹é…ç½®å®Œæˆ: {model_type}, scale={scale}")
        
        return (model_data,)


class FrameInterpolator:
    """
    é€šç”¨è¡¥å¸§èŠ‚ç‚¹
    æ”¯æŒ GMFSS / RIFE æ¨¡å‹ï¼Œæˆ–ä½¿ç”¨çº¿æ€§æ’å€¼
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "frames": ("IMAGE",),
                "multiplier": (["2x", "4x", "8x"], {
                    "default": "2x",
                    "tooltip": "è¡¥å¸§å€æ•°"
                }),
            },
            "optional": {
                "vfi_model": ("VFI_MODEL", {
                    "tooltip": "è§†é¢‘è¡¥å¸§æ¨¡å‹ (æ¥è‡ªGMFSSModelLoader)"
                }),
                "fallback_mode": (["linear", "rife"], {
                    "default": "linear",
                    "tooltip": "æ— æ¨¡å‹æ—¶çš„å›é€€æ¨¡å¼"
                }),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("frames",)
    FUNCTION = "interpolate"
    CATEGORY = "None-upup"

    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
        self._gmfss_model = None
        self._gmfss_config = None

    def interpolate(self, frames, multiplier, vfi_model=None, fallback_mode="linear"):
        mult = {"2x": 2, "4x": 4, "8x": 8}[multiplier]
        
        if vfi_model is not None:
            if vfi_model.get("type") == "gmfss":
                result = self._gmfss_interpolate(frames, mult, vfi_model)
            else:
                result = self._model_interpolate(frames, mult, vfi_model)
        elif fallback_mode == "rife":
            result = self._rife_interpolate(frames, mult)
        else:
            result = self._linear_interpolate(frames, mult)
        
        return (result,)

    def _gmfss_interpolate(self, frames: torch.Tensor, mult: int, config: dict) -> torch.Tensor:
        """GMFSSå…‰æµè¡¥å¸§"""
        N, H, W, C = frames.shape
        scale = config.get("scale", 1.0)
        device = config.get("device", self.device)
        dtype = config.get("dtype", self.dtype)
        model_path = config.get("model_path")
        model_type = config.get("model_type", "union")
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹
        if self._gmfss_model is None or self._gmfss_config != model_path:
            self._gmfss_model = self._load_gmfss(model_path, model_type, device)
            self._gmfss_config = model_path
        
        if self._gmfss_model is None:
            print("[FrameInterpolator] GMFSSåŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°çº¿æ€§æ’å€¼")
            return self._linear_interpolate(frames, mult)
        
        # è½¬æ¢æ ¼å¼ [N, C, H, W] å¹¶å½’ä¸€åŒ–
        frames_gpu = frames.permute(0, 3, 1, 2).to(device, dtype=dtype)
        
        # ç¡®ä¿å°ºå¯¸å¯è¢«32æ•´é™¤ (GMFSSè¦æ±‚)
        ph = ((H - 1) // 32 + 1) * 32
        pw = ((W - 1) // 32 + 1) * 32
        padding = (0, pw - W, 0, ph - H)
        frames_padded = F.pad(frames_gpu, padding, mode='replicate')
        
        interpolated = []
        
        for i in range(N - 1):
            f0 = frames_padded[i:i+1]
            f1 = frames_padded[i+1:i+2]
            
            interpolated.append(f0[:, :, :H, :W].cpu())
            
            # ç”Ÿæˆä¸­é—´å¸§
            for t_idx in range(1, mult):
                timestep = t_idx / mult
                with torch.no_grad():
                    # GMFSSæ¥å£: inference(img0, img1, timestep, scale)
                    mid = self._gmfss_model.inference(f0, f1, timestep, scale)
                    mid = mid[:, :, :H, :W]  # è£å‰ªå›åŸå°ºå¯¸
                interpolated.append(mid.cpu())
            
            # å®šæœŸæ¸…ç†æ˜¾å­˜
            if i % 10 == 0:
                torch.cuda.empty_cache()
        
        # æ·»åŠ æœ€åä¸€å¸§
        interpolated.append(frames_padded[-1:, :, :H, :W].cpu())
        
        # åˆå¹¶ç»“æœ
        result = torch.cat(interpolated, dim=0)
        result = result.permute(0, 2, 3, 1).to(torch.float32)
        return torch.clamp(result, 0, 1)

    def _load_gmfss(self, model_path: str, model_type: str, device):
        """åŠ è½½GMFSSæ¨¡å‹"""
        try:
            # å°è¯•åŠ è½½æœ¬åœ°GMFSSå®ç°
            gmfss_dir = os.path.dirname(model_path)
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å‹
            if model_type == "union":
                # Unionæ¨¡å‹
                try:
                    from model.GMFSS_union import Model
                except ImportError:
                    # å°è¯•ä»ComfyUIæ’ä»¶åŠ è½½
                    try:
                        from custom_nodes.ComfyUI_VFI.gmfss_union import Model
                    except ImportError:
                        Model = self._create_gmfss_wrapper()
            else:
                # åŸºç¡€GMFSSæ¨¡å‹
                try:
                    from model.GMFSS import Model
                except ImportError:
                    try:
                        from custom_nodes.ComfyUI_VFI.gmfss import Model
                    except ImportError:
                        Model = self._create_gmfss_wrapper()
            
            if Model is None:
                return None
            
            model = Model()
            model.load_model(model_path, -1)  # -1 = auto select GPU
            model.eval()
            model.device()
            
            print(f"[FrameInterpolator] GMFSSæ¨¡å‹åŠ è½½æˆåŠŸ: {model_type}")
            return model
            
        except Exception as e:
            print(f"[FrameInterpolator] GMFSSåŠ è½½å¤±è´¥: {e}")
            return None

    def _create_gmfss_wrapper(self):
        """åˆ›å»ºGMFSSåŒ…è£…å™¨ (å½“æ— æ³•å¯¼å…¥åŸå§‹æ¨¡å‹æ—¶)"""
        # è¿”å›Noneï¼Œå°†å›é€€åˆ°çº¿æ€§æ’å€¼
        print("[FrameInterpolator] GMFSSæ¨¡å‹å®šä¹‰æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£…GMFSS_Fortuna")
        return None

    def _model_interpolate(self, frames: torch.Tensor, mult: int, model_data: dict) -> torch.Tensor:
        """é€šç”¨æ¨¡å‹è¡¥å¸§"""
        model = model_data.get("model")
        if model is None:
            return self._linear_interpolate(frames, mult)
        
        N = frames.shape[0]
        frames_gpu = frames.permute(0, 3, 1, 2).to(self.device, dtype=self.dtype)
        
        interpolated = []
        for i in range(N - 1):
            f0 = frames_gpu[i:i+1]
            f1 = frames_gpu[i+1:i+2]
            
            interpolated.append(f0.cpu())
            for t in range(1, mult):
                with torch.no_grad():
                    mid = model(f0, f1, t / mult)
                interpolated.append(mid.cpu())
        
        interpolated.append(frames_gpu[-1:].cpu())
        
        result = torch.cat(interpolated, dim=0)
        result = result.permute(0, 2, 3, 1).to(torch.float32)
        return torch.clamp(result, 0, 1)

    def _rife_interpolate(self, frames: torch.Tensor, mult: int) -> torch.Tensor:
        """RIFEå…‰æµè¡¥å¸§ (å›é€€æ¨¡å¼)"""
        try:
            from custom_nodes.ComfyUI_Frame_Interpolation.rife_model import RIFE
            model = RIFE().to(self.device)
            model.eval()
        except:
            print("[FrameInterpolator] RIFEä¸å¯ç”¨ï¼Œå›é€€åˆ°çº¿æ€§æ’å€¼")
            return self._linear_interpolate(frames, mult)
        
        N = frames.shape[0]
        frames_gpu = frames.permute(0, 3, 1, 2).to(self.device, dtype=self.dtype)
        
        interpolated = []
        for i in range(N - 1):
            f0 = frames_gpu[i:i+1]
            f1 = frames_gpu[i+1:i+2]
            
            interpolated.append(f0.cpu())
            for t in range(1, mult):
                with torch.no_grad():
                    mid = model(f0, f1, t / mult)
                interpolated.append(mid.cpu())
        
        interpolated.append(frames_gpu[-1:].cpu())
        
        result = torch.cat(interpolated, dim=0)
        result = result.permute(0, 2, 3, 1).to(torch.float32)
        return torch.clamp(result, 0, 1)

    def _linear_interpolate(self, frames: torch.Tensor, mult: int) -> torch.Tensor:
        """çº¿æ€§æ’å€¼è¡¥å¸§"""
        N = frames.shape[0]
        interpolated = []
        
        for i in range(N - 1):
            f0, f1 = frames[i], frames[i + 1]
            interpolated.append(f0)
            for t in range(1, mult):
                alpha = t / mult
                interpolated.append((1 - alpha) * f0 + alpha * f1)
        
        interpolated.append(frames[-1])
        return torch.stack(interpolated, dim=0)


# ComfyUIèŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "VideoCinematicProcessor": VideoCinematicProcessor,
    "VideoFrameExtractor": VideoFrameExtractor,
    "VideoFrameComposer": VideoFrameComposer,
    "GMFSSModelLoader": GMFSSModelLoader,
    "FrameInterpolator": FrameInterpolator,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "VideoCinematicProcessor": "ğŸ¬ Video Cinematic Processor",
    "VideoFrameExtractor": "ğŸ“½ï¸ Video Frame Extractor",
    "VideoFrameComposer": "ğŸ¥ Video Frame Composer",
    "GMFSSModelLoader": "ğŸ”„ GMFSS Model Loader",
    "FrameInterpolator": "â© Frame Interpolator",
}
